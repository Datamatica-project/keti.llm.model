#!/bin/bash

# ğŸ”§ Gemma ëª¨ë¸ì„ ìœ„í•œ ìµœì í™”ëœ vLLM ì„¤ì •

# í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export AWS_ACCESS_KEY_ID=minio
export AWS_SECRET_ACCESS_KEY=miniostorage
export TOKENIZERS_PARALLELISM=false

MODEL_NAME="unsloth/gemma-3-4b-it"

echo "ğŸš€ Gemma-3-4B vLLM ì„œë²„ ì‹œì‘ ì¤‘..."
echo "ëª¨ë¸: $MODEL_NAME"

# vLLM ì‹¤í–‰ - Gemma EOS í† í° ì²˜ë¦¬ ìµœì í™”
python3 -m vllm.entrypoints.openai.api_server \
  --model ${MODEL_NAME} \
  --dtype bfloat16 \
  --host 0.0.0.0 \
  --port 8000 \
  --max-model-len 2048 \
  --max-num-seqs 16 \
  --enable-force-include-usage \
  --disable-log-stats \
  --trust-remote-code \
  --tokenizer ${MODEL_NAME} \
  --served-model-name "unsloth/gemma-3-4b-it" \
  --gpu-memory-utilization 0.8 \
  --enforce-eager
