from .reranker import load_reranker, rerank_with_bge
from .search import vector_search
from dto.routings import RoutingResult, RouteType

from .buffer import save_session_memory
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from transformers import AutoTokenizer

from typing import Dict, Any, List


reranker = load_reranker()

tokenizer = AutoTokenizer.from_pretrained("unsloth/gemma-3-4b-it", trust_remote_code=True)


def count_tokens(messages: List) -> int:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
    total_tokens = 0
    for message in messages:
        if hasattr(message, 'content'):
            total_tokens += len(tokenizer.encode(message.content))
        else:
            total_tokens += len(tokenizer.encode(str(message)))
    return total_tokens


def route_query(query: str) -> RoutingResult:
    """ì§ˆë¬¸ ë¼ìš°íŒ… íŒë‹¨"""

    llm = ChatOpenAI(
        model_name="unsloth/gemma-3-4b-it",
        openai_api_base="http://localhost:8000/v1",
        max_tokens=50,
        temperature=0,
        openai_api_key="sk-fake-key"
    )

    prompt = f"""ì´ ì§ˆë¬¸ì´ ë†ì—… ì „ë¬¸ ì§€ì‹/ë¬¸ì„œ ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ì„¸ìš”.

ì§ˆë¬¸: "{query}"

ë†ì—… ê¸°ìˆ , ì‘ë¬¼ ì¬ë°°, ë³‘í•´ì¶©, ë†ì•½ ë“± ì „ë¬¸ ì •ë³´ê°€ í•„ìš”í•˜ë©´ "ë†ì—…ê²€ìƒ‰"
ì¼ë°˜ ëŒ€í™”, ì¸ì‚¬, ê°ì‚¬ ë“±ì€ "ì¼ë°˜ëŒ€í™”"

ë‹µë³€: """

    try:
        response = llm.invoke(prompt)
        content = response.content.strip()

        if "ë†ì—…ê²€ìƒ‰" in content:
            route: RouteType = "document_search"
            reasoning = "ë†ì—… ì „ë¬¸ ì§€ì‹ í•„ìš”"
        else:
            route = "general_chat"
            reasoning = "ì¼ë°˜ ëŒ€í™”"

        return RoutingResult(route=route, reasoning=reasoning)

    except Exception as e:
        # ì˜¤ë¥˜ì‹œ ì•ˆì „í•œ ê¸°ë³¸ê°’
        return RoutingResult(route="document_search", reasoning=f"ë¼ìš°íŒ… ì˜¤ë¥˜: {str(e)}")


def generate_response(query: str, session_id: str) -> Dict[str, Any]:
    """ë™ì  ë¼ìš°íŒ…ì´ ì ìš©ëœ ì‘ë‹µ ìƒì„±"""

    # Redis ê¸°ë°˜ ë²„í¼ ë©”ëª¨ë¦¬ ìƒì„±

    memory = save_session_memory(session_id, "redis://192.168.0.150:6379")
    if memory is None:
        return {"error": "ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨"}

    input_query = len(tokenizer.encode(query))


    routing: RoutingResult = route_query(query)
    print(f"ë¼ìš°íŒ…: {routing['route']} - {routing['reasoning']}")

    # ë¼ìš°íŒ… ê²°ê³¼ì— ë”°ë¥¸ ì²˜ë¦¬
    if routing["route"] == "document_search":
        print(f"ë†ì—… ê²€ìƒ‰ ìˆ˜í–‰: {query}")
        vector_results = vector_search(query, top_k=8)
        reranked = rerank_with_bge(query, vector_results, reranker, top_k=5)

        references = [
            {
                "document": doc.get("document"),
                "text": doc.get("text"),
                "score": score
            }
            for doc, score in reranked if score > 0.5
        ]
    else:
        print(f"ì¼ë°˜ ëŒ€í™”: {query}")
        references = []

    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    if not references:
        prompt = f"""ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”.:

[ì§ˆë¬¸]
{query}
"""
    else:
        context_docs = references
        context = "\n".join([ref["text"] for ref in context_docs])
        prompt = f"""ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”.:

[ë¬¸ì„œ ìš”ì•½]
{context}

[ì§ˆë¬¸]
{query}
"""

    # ì‘ë‹µ ìƒì„±ìš© LLM (ë¼ìš°íŒ…ìš©ê³¼ ë‹¤ë¥¸ ì„¤ì •)
    llm = ChatOpenAI(
        model_name="unsloth/gemma-3-4b-it",
        openai_api_base="http://localhost:8000/v1",
        max_tokens=1024,
        temperature=0.1,
        openai_api_key="sk-fake-key"
    )

    # ì´ì „ ëŒ€í™” ë©”ì‹œì§€ ë¶ˆëŸ¬ì˜¤ê¸°
    previous_messages = memory.load_memory_variables({}).get("chat_history", [])

    # ìƒˆ ë©”ì‹œì§€ ì¶”ê°€
    current_messages = [
        HumanMessage(content="ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ëŠ” ì „ë¬¸ ë†ì—… ìƒë‹´ê°€ì…ë‹ˆë‹¤. ì ˆëŒ€ë¡œ ì™¸êµ­ì–´ë¥¼ ì„ì§€ ë§ê³ , ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\n\n" + prompt)
    ]

    all_messages = previous_messages + current_messages

    print(f"DEBUG: ì´ˆê¸° í† í° ìˆ˜: {count_tokens(all_messages)}")
    print(f"DEBUG: previous_messages ê¸¸ì´: {len(previous_messages)}")

    max_tokens = 2500

    while count_tokens(all_messages) > max_tokens and len(previous_messages) > 0:
        # ê¸°ì¡´
        previous_messages.pop(0)

        # ìˆ˜ì •: ìˆœì„œê°€ ê¹¨ì§€ì§€ ì•Šê²Œ 2ê°œì”© ì‚­ì œ (user+assistant ìŒìœ¼ë¡œ)
        if len(previous_messages) >= 2:
            previous_messages.pop(0)  # user ì‚­ì œ
            previous_messages.pop(0)  # assistant ì‚­ì œ
        else:
            previous_messages.clear()  # ë‚¨ì€ ê²Œ 1ê°œë©´ ì „ì²´ ì‚­ì œ

        all_messages = previous_messages + current_messages
        print(f"ğŸ—‘ëŒ€í™” ìŒ ì‚­ì œ, í˜„ì¬ í† í°: {count_tokens(all_messages)}")

    print(f"DEBUG: ìµœì¢… í† í° ìˆ˜: {count_tokens(all_messages)}")

    response = llm.invoke(all_messages)

    # ì‘ë‹µì„ ë©”ëª¨ë¦¬ì— ì¶”ê°€

    memory.save_context(
        {"input": query},
        {"output": response.content}
    )

    token_usage = response.response_metadata.get("token_usage", {})

    return {
        "answer": response.content,
        "input_tokens": input_query,
        "completion_tokens": token_usage.get("completion_tokens", 0),
        "references": references[0]["document"] if references else "",
        "rank": references
    }