<<<<<<< HEAD
from openai import OpenAI
import json
import time
import os

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
client = OpenAI(
    api_key='api')


def generate_quality_qa_batch(text, num_questions=25):
    """ë°°ì¹˜ìš© ê³ í’ˆì§ˆ QA ìƒì„± (ìµœëŒ€ 30ê°œê¹Œì§€)"""

    prompt = f"""
ë‹¤ìŒ ë†ì—… ë¬¸ì„œì—ì„œ {num_questions}ê°œì˜ ì „ë¬¸ì ì¸ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë§Œë“œì„¸ìš”.

ë¬¸ì„œ:
{text}

ì¡°ê±´:
1. ë‹µë³€ì€ ë°˜ë“œì‹œ 500ì ì´ìƒ
2. êµ¬ì²´ì  ìˆ˜ì¹˜, ë°©ë²•, ì‹œê¸° í¬í•¨
3. ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ ë‚´ìš©
4. ë‹¤ìŒ êµ¬ì¡°ë¡œ ë‹µë³€ ì‘ì„±:
   - ì „ë¬¸ê°€ ë„ì…ë¶€: "ë†ì—… ì „ë¬¸ê°€ë¡œì„œ ìƒì„¸íˆ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
   - í•µì‹¬ ë‚´ìš© (ë¬¸ì„œ ê¸°ë°˜)
   - **êµ¬ì²´ì  ë°©ë²•**: ë‹¨ê³„ë³„ ì‹¤í–‰ ë°©ì•ˆ
   - **ì‹¤ë¬´ íŒ**: í˜„ì¥ ë…¸í•˜ìš°
   - **ì£¼ì˜ì‚¬í•­**: ì‹¤ì œ ì£¼ì˜ì 

JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥:
[
  {{
    "QUESTION": "êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì§ˆë¬¸",
    "ANSWER": "ë†ì—… ì „ë¬¸ê°€ë¡œì„œ ìƒì„¸íˆ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\\n\\n[í•µì‹¬ ë‚´ìš©]\\n\\n**êµ¬ì²´ì  ë°©ë²•:**\\n- ë°©ë²•ë“¤\\n\\n**ì‹¤ë¬´ íŒ:**\\n- íŒë“¤\\n\\n**ì£¼ì˜ì‚¬í•­:**\\n- ì£¼ì˜ì ë“¤"
  }}
]
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ GPT í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return None


def parse_json_safe(raw_response):
    """ì•ˆì „í•œ JSON íŒŒì‹±"""
    try:
        # ë§ˆí¬ë‹¤ìš´ ì œê±°
        clean_response = raw_response.replace("```json", "").replace("```", "").strip()
        qa_list = json.loads(clean_response)
        return qa_list
    except json.JSONDecodeError as e:
        print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None


def batch_generate_qa(text, total_questions=1000, batch_size=25, save_file="batch_qa_results.json"):
    """ë°°ì¹˜ ì²˜ë¦¬ë¡œ ëŒ€ëŸ‰ QA ìƒì„±"""

    print("ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ QA ìƒì„± ì‹œì‘")
    print(f"ğŸ¯ ëª©í‘œ: {total_questions}ê°œ")
    print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size}ê°œ")
    print(f"ğŸ“„ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}ì")
    print("=" * 60)

    # ë°°ì¹˜ ê³„ì‚°
    num_batches = (total_questions + batch_size - 1) // batch_size
    print(f"ğŸ“Š ì´ {num_batches}ë²ˆì˜ ë°°ì¹˜ í˜¸ì¶œ ì˜ˆì •")

    all_qa = []
    success_count = 0
    fail_count = 0

    for batch_num in range(num_batches):
        # ë‚¨ì€ ì§ˆë¬¸ ìˆ˜ ê³„ì‚°
        remaining = min(batch_size, total_questions - len(all_qa))

        print(f"\nğŸ”„ ë°°ì¹˜ {batch_num + 1}/{num_batches}")
        print(f"   ìƒì„±í•  QA: {remaining}ê°œ")
        print(f"   ëˆ„ì  ìƒì„±: {len(all_qa)}ê°œ")

        # ë°°ì¹˜ ì‹¤í–‰
        start_time = time.time()
        raw_response = generate_quality_qa_batch(text, remaining)
        end_time = time.time()

        if raw_response:
            # JSON íŒŒì‹±
            qa_batch = parse_json_safe(raw_response)

            if qa_batch:
                # í’ˆì§ˆ ê²€ì¦
                valid_qa = []
                for qa in qa_batch:
                    if validate_qa_quality(qa):
                        valid_qa.append(qa)

                all_qa.extend(valid_qa)
                success_count += 1

                print(f"   âœ… ì„±ê³µ: {len(valid_qa)}ê°œ ìƒì„±")
                print(f"   â±ï¸ ì†Œìš”ì‹œê°„: {end_time - start_time:.1f}ì´ˆ")

                # ì¤‘ê°„ ì €ì¥ (50ê°œë§ˆë‹¤)
                if len(all_qa) % 50 == 0:
                    save_intermediate(all_qa, f"temp_qa_{len(all_qa)}.json")
            else:
                fail_count += 1
                print(f"   âŒ íŒŒì‹± ì‹¤íŒ¨")
        else:
            fail_count += 1
            print(f"   âŒ GPT í˜¸ì¶œ ì‹¤íŒ¨")

        # ì§„í–‰ë¥  í‘œì‹œ
        progress = (batch_num + 1) / num_batches * 100
        print(f"   ğŸ“ˆ ì§„í–‰ë¥ : {progress:.1f}%")

        # API ì œí•œ ë°©ì§€ ëŒ€ê¸°
        if batch_num < num_batches - 1:
            wait_time = 3
            print(f"   â³ {wait_time}ì´ˆ ëŒ€ê¸°...")
            time.sleep(wait_time)

    # ìµœì¢… ê²°ê³¼
    print(f"\nğŸ‰ ë°°ì¹˜ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“Š ìµœì¢… í†µê³„:")
    print(f"   ëª©í‘œ: {total_questions}ê°œ")
    print(f"   ì‹¤ì œ ìƒì„±: {len(all_qa)}ê°œ")
    print(f"   ì„±ê³µë¥ : {len(all_qa) / total_questions * 100:.1f}%")
    print(f"   ì„±ê³µ ë°°ì¹˜: {success_count}/{num_batches}")
    print(f"   ì‹¤íŒ¨ ë°°ì¹˜: {fail_count}/{num_batches}")

    # í’ˆì§ˆ í†µê³„
    if all_qa:
        avg_length = sum(len(qa['ANSWER']) for qa in all_qa) / len(all_qa)
        long_enough = sum(1 for qa in all_qa if len(qa['ANSWER']) >= 500)

        print(f"\nğŸ“Š í’ˆì§ˆ í†µê³„:")
        print(f"   í‰ê·  ë‹µë³€ ê¸¸ì´: {avg_length:.0f}ì")
        print(f"   500ì ì´ìƒ: {long_enough}/{len(all_qa)}ê°œ ({long_enough / len(all_qa) * 100:.1f}%)")

    # ìµœì¢… ì €ì¥
    with open(save_file, 'w', encoding='utf-8') as f:
        json.dump(all_qa, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ {save_file}ì— ì €ì¥ ì™„ë£Œ")

    return all_qa


def validate_qa_quality(qa):
    """QA í’ˆì§ˆ ê²€ì¦"""
    question = qa.get('QUESTION', '')
    answer = qa.get('ANSWER', '')

    # ê¸°ë³¸ ê²€ì¦
    if not question or not answer:
        return False

    # ê¸¸ì´ ê²€ì¦
    if len(answer) < 500:
        return False

    # êµ¬ì¡° ê²€ì¦
    if 'ë†ì—… ì „ë¬¸ê°€' not in answer:
        return False

    # ì„¹ì…˜ ê²€ì¦
    required_sections = ['êµ¬ì²´ì ', 'ë°©ë²•', 'íŒ', 'ì£¼ì˜']
    section_count = sum(1 for section in required_sections if section in answer)

    if section_count < 3:
        return False

    return True


def save_intermediate(qa_list, filename):
    """ì¤‘ê°„ ì €ì¥"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(qa_list, f, ensure_ascii=False, indent=2)
    print(f"   ğŸ’¾ ì¤‘ê°„ ì €ì¥: {filename}")


def chunk_and_generate(long_text, total_questions=1000, chunk_size=5000):
    """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ QA ìƒì„±"""

    print("âœ‚ï¸ í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  + QA ìƒì„±")
    print(f"ğŸ“„ ì „ì²´ í…ìŠ¤íŠ¸: {len(long_text)}ì")
    print(f"ğŸ“ ì²­í¬ í¬ê¸°: {chunk_size}ì")

    # í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• 
    chunks = []
    for i in range(0, len(long_text), chunk_size):
        chunk = long_text[i:i + chunk_size]
        if len(chunk) > 1000:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸
            chunks.append(chunk)

    print(f"ğŸ“Š ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

    # ì²­í¬ë‹¹ QA ìˆ˜ ê³„ì‚°
    qa_per_chunk = total_questions // len(chunks)
    remaining_qa = total_questions % len(chunks)

    print(f"ğŸ¯ ì²­í¬ë‹¹ {qa_per_chunk}ê°œ QA (ë§ˆì§€ë§‰ ì²­í¬ +{remaining_qa}ê°œ)")

    all_qa = []

    for i, chunk in enumerate(chunks):
        chunk_qa_count = qa_per_chunk + (remaining_qa if i == len(chunks) - 1 else 0)

        print(f"\nğŸ“ ì²­í¬ {i + 1}/{len(chunks)} ì²˜ë¦¬")
        print(f"   ê¸¸ì´: {len(chunk)}ì")
        print(f"   ëª©í‘œ QA: {chunk_qa_count}ê°œ")

        # ì²­í¬ë³„ ë°°ì¹˜ ìƒì„±
        chunk_qa = batch_generate_qa(
            chunk,
            total_questions=chunk_qa_count,
            batch_size=25,
            save_file=f"chunk_{i + 1}_qa.json"
        )

        all_qa.extend(chunk_qa)
        print(f"   âœ… ì²­í¬ {i + 1} ì™„ë£Œ: {len(chunk_qa)}ê°œ")

    # ì „ì²´ ê²°ê³¼ ì €ì¥
    final_file = "final_all_chunks_qa.json"
    with open(final_file, 'w', encoding='utf-8') as f:
        json.dump(all_qa, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ ì „ì²´ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ {len(all_qa)}ê°œ QA ìƒì„±")
    print(f"ğŸ’¾ {final_file}ì— ìµœì¢… ì €ì¥")

    return all_qa


def test_batch_system():
    """ë°°ì¹˜ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""

    sample_text = """
    í† ë§ˆí†  ì¬ë°°ì—ì„œ ë¬¼ ê´€ë¦¬ëŠ” ë§¤ìš° ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤. í† ë§ˆí† ëŠ” ë¿Œë¦¬ê°€ ë¹„êµì  ì–•ê²Œ ë¶„í¬í•˜ë¯€ë¡œ 
    í‘œí† ì¸µì˜ ìˆ˜ë¶„ ê´€ë¦¬ì— íŠ¹ë³„í•œ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. ê³¼ìŠµí•  ê²½ìš° ë¿Œë¦¬ì©ìŒë³‘, ì—­ë³‘ ë“±ì˜ 
    í† ì–‘ì „ì—¼ì„± ë³‘í•´ê°€ ë°œìƒí•˜ê¸° ì‰½ê³ , ë°˜ëŒ€ë¡œ ê±´ì¡°í•  ê²½ìš°ì—ëŠ” ì—´ê³¼(ê³¼ì‹¤ í„°ì§) í˜„ìƒì´ 
    ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ìƒìœ¡ ë‹¨ê³„ë³„ë¡œ ë³´ë©´, ì •ì‹ í›„ í™œì°©ê¸°ì—ëŠ” í† ì–‘ ìˆ˜ë¶„ì„ 60-70% ìˆ˜ì¤€ìœ¼ë¡œ ìœ ì§€í•˜ê³ , 
    ê°œí™”ì°©ê³¼ê¸°ì—ëŠ” ì ë‹¹í•œ ìˆ˜ë¶„ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ì£¼ì–´ ì°©ê³¼ë¥¼ ì´‰ì§„ì‹œí‚¤ë©°, ê³¼ì‹¤ ë¹„ëŒ€ê¸°ì—ëŠ” 
    ì¶©ë¶„í•œ ìˆ˜ë¶„ì„ ê³µê¸‰í•˜ì—¬ ê³¼ì‹¤ì˜ í¬ê¸°ì™€ í’ˆì§ˆì„ í–¥ìƒì‹œì¼œì•¼ í•©ë‹ˆë‹¤.

    ê´€ìˆ˜ ì‹œê¸°ëŠ” ì˜¤ì „ 8-10ì‹œê²½ì´ ê°€ì¥ ì ì ˆí•˜ë©°, ì´ë•Œ ê´€ìˆ˜í•˜ë©´ í•˜ë£¨ ì¢…ì¼ ì¶©ë¶„í•œ 
    ì¦ì‚°ì‘ìš©ì„ í†µí•´ ì–‘ë¶„ í¡ìˆ˜ê°€ ì›í™œí•´ì§‘ë‹ˆë‹¤. ì ì ê´€ìˆ˜ ì‹œìŠ¤í…œì„ í™œìš©í•˜ë©´ 
    ë¬¼ì˜ ì´ìš©íš¨ìœ¨ì„ ë†’ì´ê³  ë³‘í•´ ë°œìƒì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ì‹œì„¤ì¬ë°°ì—ì„œëŠ” í™˜ê¸°ì™€ ì˜¨ë„ ê´€ë¦¬ê°€ ë³‘í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ê³¼ìŠµí•œ í™˜ê²½ì—ì„œëŠ” 
    ì¿ë¹›ê³°íŒ¡ì´ë³‘, ì—­ë³‘ ë“±ì´ ë°œìƒí•˜ê¸° ì‰¬ìš°ë¯€ë¡œ ì ì ˆí•œ í™˜ê¸°ë¥¼ í†µí•´ ìŠµë„ë¥¼ ì¡°ì ˆí•´ì•¼ í•©ë‹ˆë‹¤.
    ì˜¨ë„ëŠ” ì£¼ê°„ 25-28Â°C, ì•¼ê°„ 15-18Â°Cë¡œ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
    """ * 10  # í…ìŠ¤íŠ¸ í™•ì¥

    print("ğŸ§ª ë°°ì¹˜ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (50ê°œ QA)")

    # í…ŒìŠ¤íŠ¸: 50ê°œ QA ìƒì„±
    result = batch_generate_qa(
        text=sample_text,
        total_questions=50,
        batch_size=25,
        save_file="test_batch_50qa.json"
    )

    return result


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ê¸°ë°˜ ëŒ€ëŸ‰ QA ìƒì„± ì‹œìŠ¤í…œ")
    print("=" * 60)

    while True:
        print("\nğŸ“‹ ë©”ë‰´:")
        print("1. í…ŒìŠ¤íŠ¸ (50ê°œ QA)")
        print("2. ì†ŒëŸ‰ ìƒì„± (100ê°œ QA)")
        print("3. ì¤‘ëŸ‰ ìƒì„± (500ê°œ QA)")
        print("4. ëŒ€ëŸ‰ ìƒì„± (1000ê°œ QA)")
        print("5. ì²­í¬ ë¶„í•  ìƒì„± (ê¸´ í…ìŠ¤íŠ¸ìš©)")
        print("6. ì¢…ë£Œ")

        choice = input("\nì„ íƒí•˜ì„¸ìš” (1-6): ")

        if choice == "1":
            test_batch_system()

        elif choice == "2":
            text = input("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ë˜ëŠ” íŒŒì¼ ê²½ë¡œ): ")
            if os.path.exists(text):
                with open(text, 'r', encoding='utf-8') as f:
                    text = f.read()
            batch_generate_qa(text, total_questions=100, save_file="qa_100.json")

        elif choice == "3":
            text = input("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ë˜ëŠ” íŒŒì¼ ê²½ë¡œ): ")
            if os.path.exists(text):
                with open(text, 'r', encoding='utf-8') as f:
                    text = f.read()
            batch_generate_qa(text, total_questions=500, save_file="qa_500.json")

        elif choice == "4":
            text = input("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ë˜ëŠ” íŒŒì¼ ê²½ë¡œ): ")
            if os.path.exists(text):
                with open(text, 'r', encoding='utf-8') as f:
                    text = f.read()
            batch_generate_qa(text, total_questions=1000, save_file="qa_1000.json")

        elif choice == "5":
            text = input("ê¸´ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
            if os.path.exists(text):
                with open(text, 'r', encoding='utf-8') as f:
                    long_text = f.read()
                total = int(input("ì´ ìƒì„±í•  QA ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”: "))
                chunk_and_generate(long_text, total_questions=total)
            else:
                print("âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        elif choice == "6":
            print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
=======
import logging
from collections import defaultdict
from utils.storage import StorageManager
from utils.generate import (
    generate_templates_batch, get_questions_config,
    enhance_qa_dataset, filter_quality_qa, format_for_instruction_tuning
)
import os
import json

SAVE_DIR = "C:/Users/dm_ohminchan/Model/data/instrcution/"


def main():
    result = StorageManager("chunk").download()
    document_groups = defaultdict(list)

    for file in result:
        contents = file["content"]

        # ì´ì¤‘ ë¦¬ìŠ¤íŠ¸ í‰íƒ„í™”
        if isinstance(contents, list) and contents and isinstance(contents[0], list):
            contents = [chunk for sublist in contents for chunk in sublist]

        for chunk in contents:
            doc_name = chunk.get("document")
            if doc_name:
                document_groups[doc_name].append(chunk)

    # ë¬¸ì„œë³„ë¡œ content í•©ì¹˜ê¸°
    merged_documents = []
    for doc_name, chunks in document_groups.items():
        # contentë¥¼ ìˆœì„œëŒ€ë¡œ ì—°ê²°
        merged_text = "\n".join(chunk.get("content", "") for chunk in chunks)

        merged_documents.append({
            "document": doc_name,
            "merged_content": merged_text,
        })

    return merged_documents


def generate_all():
    documents = main()

    for doc in documents[6:9]:
        context = doc["merged_content"]
        doc_name = doc["document"]
        domain = "ë†ì—…"

        total_q, batch_size = get_questions_config(len(context))
        logging.info(f"ë¬¸ì„œ: {doc_name} | ê¸¸ì´: {len(context)}ì â†’ ì§ˆë¬¸ {total_q}ê°œ ìƒì„±")

        # 1. ì›ë³¸ QA ìƒì„±
        qa_list = generate_templates_batch(
            context, domain,
            total_questions=total_q,
            batch_size=batch_size
        )

        if not qa_list:
            logging.warning(f"{doc_name} ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨ â€” ìƒëµ")
            continue

        logging.info(f"{doc_name} ì›ë³¸ QA {len(qa_list)}ê°œ ìƒì„± ì™„ë£Œ")

        # 2. ë°ì´í„°ì…‹ ê°œì„ 
        logging.info(f"{doc_name} ë°ì´í„°ì…‹ ê°œì„  ì‹œì‘...")
        enhanced_data = enhance_qa_dataset(qa_list)
        logging.info(f"{doc_name} ê°œì„  ì™„ë£Œ: {len(enhanced_data)}ê°œ")

        # 3. í’ˆì§ˆ í•„í„°ë§
        quality_data = filter_quality_qa(enhanced_data, min_answer_length=200)
        logging.info(f"{doc_name} í’ˆì§ˆ í•„í„°ë§ í›„: {len(quality_data)}ê°œ")

        # 4. Instruction tuning í˜•íƒœë¡œ í¬ë§·
        final_data = format_for_instruction_tuning(quality_data)
        logging.info(f"{doc_name} ìµœì¢… ë°ì´í„°: {len(final_data)}ê°œ")

        # 5. íŒŒì¼ ì €ì¥ (3ê°€ì§€ ë²„ì „)
        base_filename = doc_name.replace('.pdf', '')

        # ì›ë³¸ ë°ì´í„° ì €ì¥
        original_file = os.path.join(SAVE_DIR, f"original_{base_filename}.json")
        with open(original_file, "w", encoding="utf-8") as f:
            json.dump(qa_list, f, ensure_ascii=False, indent=2)

        # ê°œì„ ëœ ë°ì´í„° ì €ì¥ (chat template í˜•íƒœ)
        enhanced_file = os.path.join(SAVE_DIR, f"enhanced_{base_filename}.json")
        with open(enhanced_file, "w", encoding="utf-8") as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)

        # ë‹¨ìˆœ QA í˜•íƒœë¡œë„ ì €ì¥ (í˜¸í™˜ì„±)
        simple_qa = [{"QUESTION": item["QUESTION"], "ANSWER": item["ANSWER"]} for item in final_data]
        simple_file = os.path.join(SAVE_DIR, f"qa_{base_filename}.json")
        with open(simple_file, "w", encoding="utf-8") as f:
            json.dump(simple_qa, f, ensure_ascii=False, indent=2)

        logging.info(f"{doc_name} ì €ì¥ ì™„ë£Œ:")
        logging.info(f"  - ì›ë³¸: {original_file}")
        logging.info(f"  - ê°œì„ : {enhanced_file}")
        logging.info(f"  - í˜¸í™˜: {simple_file}")

        # ê°œì„  ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥
        if final_data:
            logging.info(f"\n=== {doc_name} ê°œì„  ê²°ê³¼ ìƒ˜í”Œ ===")
            sample = final_data[0]
            logging.info(f"ì§ˆë¬¸: {sample['QUESTION']}")
            logging.info(f"ë‹µë³€: {sample['ANSWER'][:200]}...")


def generate_enhanced_only():
    """ê°œì„ ëœ ë°ì´í„°ë§Œ ìƒì„±í•˜ëŠ” ë²„ì „ (ë” ê°„ë‹¨)"""
    documents = main()

    for doc in documents[6:9]:
        context = doc["merged_content"]
        doc_name = doc["document"]
        domain = "ë†ì—…"

        total_q, batch_size = get_questions_config(len(context))
        logging.info(f"ë¬¸ì„œ: {doc_name} | ê¸¸ì´: {len(context)}ì â†’ ì§ˆë¬¸ {total_q}ê°œ ìƒì„±")

        # QA ìƒì„± ë° ì¦‰ì‹œ ê°œì„ 
        qa_list = generate_templates_batch(
            context, domain,
            total_questions=total_q,
            batch_size=batch_size
        )

        if not qa_list:
            logging.warning(f"{doc_name} ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨ â€” ìƒëµ")
            continue

        # ì›ìŠ¤í… ê°œì„  ì²˜ë¦¬
        enhanced_data = enhance_qa_dataset(qa_list)
        quality_data = filter_quality_qa(enhanced_data)
        final_data = format_for_instruction_tuning(quality_data)

        # ìµœì¢… íŒŒì¼ë§Œ ì €ì¥
        filename = os.path.join(SAVE_DIR, f"enhanced_{doc_name.replace('.pdf', '')}.json")
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)

        logging.info(f"{doc_name}: ì›ë³¸ {len(qa_list)}ê°œ â†’ ìµœì¢… {len(final_data)}ê°œ ì €ì¥ ì™„ë£Œ")


def enhance_existing_files():
    """ì´ë¯¸ ìƒì„±ëœ ì›ë³¸ íŒŒì¼ë“¤ì„ ê°œì„ í•˜ëŠ” í•¨ìˆ˜"""
    for filename in os.listdir(SAVE_DIR):
        if filename.startswith("qa_") and filename.endswith(".json") and not filename.startswith("enhanced_"):
            filepath = os.path.join(SAVE_DIR, filename)

            with open(filepath, 'r', encoding='utf-8') as f:
                original_data = json.load(f)

            logging.info(f"{filename} ê°œì„  ì‹œì‘: {len(original_data)}ê°œ")

            # ê°œì„  ì²˜ë¦¬
            enhanced_data = enhance_qa_dataset(original_data)
            quality_data = filter_quality_qa(enhanced_data)
            final_data = format_for_instruction_tuning(quality_data)

            # ê°œì„ ëœ íŒŒì¼ ì €ì¥
            enhanced_filename = filename.replace("qa_", "enhanced_")
            enhanced_filepath = os.path.join(SAVE_DIR, enhanced_filename)

            with open(enhanced_filepath, 'w', encoding='utf-8') as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)

            logging.info(f"{filename}: {len(original_data)}ê°œ â†’ {len(final_data)}ê°œë¡œ ê°œì„  ì™„ë£Œ")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(levelname)s - %(message)s')

    # ì˜µì…˜ 1: ì „ì²´ í”„ë¡œì„¸ìŠ¤ (ì›ë³¸ + ê°œì„ ëœ ë²„ì „ ëª¨ë‘ ì €ì¥)
    generate_all()

    # ì˜µì…˜ 2: ê°œì„ ëœ ë²„ì „ë§Œ ìƒì„± (ë” ê°„ë‹¨)
    # generate_enhanced_only()

    # ì˜µì…˜ 3: ê¸°ì¡´ íŒŒì¼ë“¤ì„ ê°œì„  (ì´ë¯¸ ì›ë³¸ì´ ìˆì„ ë•Œ)
    enhance_existing_files()
>>>>>>> 0c89ddde410f1f3f2d72ee223ac04c56dff24b2e
